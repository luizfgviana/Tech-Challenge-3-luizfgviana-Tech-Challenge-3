{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504cfc84",
   "metadata": {},
   "source": [
    " 1. Configura√ß√£o Inicial e Verifica√ß√£o de Ambiente (Adaptado para MacBook M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5102f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Apple Silicon (MPS) detectado! Usando acelera√ß√£o via Metal Performance Shaders.\n",
      "Dispositivo selecionado para PyTorch: mps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Verifica√ß√£o de ambiente para Apple Silicon (M-series)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"üéâ Apple Silicon (MPS) detectado! Usando acelera√ß√£o via Metal Performance Shaders.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"üéâ GPU detectada! Usando: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è Nenhuma GPU ou MPS detectado. O treinamento ser√° executado em CPU.\")\n",
    "print(f\"Dispositivo selecionado para PyTorch: {device}\\n\")\n",
    "\n",
    "# Para garantir a reprodutibilidade\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ba59e",
   "metadata": {},
   "source": [
    "2. Carregamento e Amostragem do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a5a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o dataset 'trn.json'...\n",
      "Dataset carregado com sucesso!\n",
      "\n",
      "Dataset original carregado: 2248619 observa√ß√µes\n",
      "Dataset amostrado: 100000 observa√ß√µes\n",
      "\n",
      "Primeiras 5 linhas do dataset amostrado:\n",
      "                                                     title  \\\n",
      "2183177         Yellow Box Women's Yulisa Flip Flop Sandal   \n",
      "1878879                   Nine West Women's Eastnor Sandal   \n",
      "1140718  Roma Costume Women's 1 Piece Santa's Envy-Red ...   \n",
      "1082826  1 Box of 100 Sony 1.55v Silver Oxide Watch Bat...   \n",
      "54788    Winning Angels: The 7 Fundamentals of Early St...   \n",
      "\n",
      "                                                   content  \n",
      "2183177                                                     \n",
      "1878879                                                     \n",
      "1140718                                                     \n",
      "1082826  100 Sony 377 Watch Batteries Silver Oxide, Mer...  \n",
      "54788    \"Winning Angels is a superbly organised and in...  \n",
      "\n",
      "Informa√ß√µes do dataset amostrado:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 100000 entries, 2183177 to 1456581\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   title    100000 non-null  object\n",
      " 1   content  100000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Defina o caminho para o seu arquivo de dados\n",
    "data_file_path = 'trn.json'\n",
    "\n",
    "print(f\"Carregando o dataset '{data_file_path}'...\")\n",
    "try:\n",
    "    \n",
    "    df = pd.read_json(data_file_path, lines=True)\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dataset: {e}\")\n",
    "    print(\"Verifique o caminho do arquivo e o formato. Se n√£o for JSON Lines, remova `lines=True` ou ajuste o m√©todo de leitura.\")\n",
    "    # Crie um DataFrame de exemplo para continuar o notebook se houver erro no carregamento real\n",
    "    data = {\n",
    "        'title': ['Boneca Interativa Beb√™', 'Kit Maquiagem Infantil Segura', 'Livro de Contos M√°gicos', 'Rob√¥ de Brinquedo Program√°vel', 'Carrinho de Controle Remoto'],\n",
    "        'content': [\n",
    "            'Minha filha adorou esta boneca! Ela interage, ri e at√© pede comida. Feita com materiais super seguros para crian√ßas pequenas, sem pe√ßas que se soltam. Perfeita para idades de 2 a 5 anos.',\n",
    "            'Cores vibrantes e f√°cil de aplicar. Minha neta se sente uma estrela! Testado dermatologicamente e totalmente seguro, sem causar alergias. Vem com um estojo pr√°tico para guardar.',\n",
    "            'Hist√≥rias cativantes e ilustra√ß√µes lindas. A cada p√°gina, uma nova aventura. Ideal para leitura antes de dormir, estimula a imagina√ß√£o. Capa dura e p√°ginas resistentes.',\n",
    "            'Este rob√¥ √© incr√≠vel! F√°cil de programar com o aplicativo e executa comandos complexos. Horas de divers√£o para crian√ßas maiores. √ìtimo para introduzir conceitos de codifica√ß√£o.',\n",
    "            'O carrinho tem uma velocidade impressionante e √≥tima durabilidade. Bateria de longa dura√ß√£o e controle preciso. Excelente presente para entusiastas de carros de todas as idades.'\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Usando um DataFrame de exemplo para demonstra√ß√£o.\")\n",
    "\n",
    "\n",
    "# Selecionar as colunas 'title' e 'content'\n",
    "df_filtered = df[['title', 'content']]\n",
    "\n",
    "# Criar uma amostra aleat√≥ria de 100.000 observa√ß√µes\n",
    "sample_size = min(100000, len(df_filtered))\n",
    "df_sample = df_filtered.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset original carregado: {len(df)} observa√ß√µes\")\n",
    "print(f\"Dataset amostrado: {len(df_sample)} observa√ß√µes\")\n",
    "print(\"\\nPrimeiras 5 linhas do dataset amostrado:\")\n",
    "print(df_sample.head())\n",
    "print(\"\\nInforma√ß√µes do dataset amostrado:\")\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7333a",
   "metadata": {},
   "source": [
    "3. Prepara√ß√£o dos Dados para Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividindo dataset em treinamento e valida√ß√£o...\n",
      "Aplicando fun√ß√£o de formata√ß√£o e removendo colunas desnecess√°rias nos datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95000/95000 [00:00<00:00, 283796.35 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 265465.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando exemplos vazios ap√≥s a formata√ß√£o...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58857/58857 [00:00<00:00, 1026133.63 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3017/3017 [00:00<00:00, 773437.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de treinamento formatado e filtrado: 58857 exemplos\n",
      "Dataset de valida√ß√£o formatado e filtrado: 3017 exemplos\n",
      "\n",
      "Exemplo de prompt formatado no dataset final (train_dataset[0]['text'][0]):\n",
      "<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples['content'])):\n",
    "        \n",
    "        content = str(examples['content'][i] if examples['content'][i] is not None else \"\").strip()\n",
    "        title = str(examples['title'][i] if examples['title'][i] is not None else \"\").strip()\n",
    "        if not content or not title:\n",
    "            continue\n",
    "        prompt = (\n",
    "            f\"<s>[INST] Analise o seguinte coment√°rio/descri√ß√£o de produto: {content} [/INST] \"\n",
    "            f\"O t√≠tulo do produto que melhor corresponde a essa descri√ß√£o √©: {title}</s>\"\n",
    "        )\n",
    "        output_texts.append(prompt)\n",
    "    return {\"text\": output_texts}\n",
    "\n",
    "# Converte o DataFrame para um objeto Dataset do Hugging Face\n",
    "# print(\"Convertendo DataFrame para Dataset...\")\n",
    "dataset = Dataset.from_pandas(df_sample)\n",
    "\n",
    "# Dividir em conjuntos de treinamento e valida√ß√£o\n",
    "print(\"Dividindo dataset em treinamento e valida√ß√£o...\")\n",
    "train_test_split = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(\"Aplicando fun√ß√£o de formata√ß√£o e removendo colunas desnecess√°rias nos datasets...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "# Filtra quaisquer exemplos que possam ter resultado em 'text' vazio ap√≥s a formata√ß√£o\n",
    "print(\"Filtrando exemplos vazios ap√≥s a formata√ß√£o...\")\n",
    "train_dataset = train_dataset.filter(lambda x: len(x['text']) > 0)\n",
    "eval_dataset = eval_dataset.filter(lambda x: len(x['text']) > 0)\n",
    "\n",
    "print(f\"Dataset de treinamento formatado e filtrado: {len(train_dataset)} exemplos\")\n",
    "print(f\"Dataset de valida√ß√£o formatado e filtrado: {len(eval_dataset)} exemplos\")\n",
    "\n",
    "# Exibir um exemplo formatado do dataset j√° processado\n",
    "print(\"\\nExemplo de prompt formatado no dataset final (train_dataset[0]['text'][0]):\")\n",
    "if len(train_dataset) > 0:\n",
    "    print(train_dataset[0]['text'][0])\n",
    "else:\n",
    "    print(\"O dataset de treinamento est√° vazio ap√≥s formata√ß√£o e filtragem. Verifique seus dados originais.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63270ee6",
   "metadata": {},
   "source": [
    "4. Carregamento do Modelo Base e Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03633924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tokenizador para o modelo 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'...\n",
      "\n",
      "Carregando modelo base 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' para mps...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo e tokenizador carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define o ID do modelo base\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Carregando tokenizador para o modelo '{model_id}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Necess√°rio para treinamento eficiente de LLMs\n",
    "\n",
    "\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "print(f\"\\nCarregando modelo base '{model_id}' para {device}...\\n\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if device == \"mps\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Configura√ß√µes do modelo para fine-tuning\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "print(\"Modelo e tokenizador carregados com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e78c4",
   "metadata": {},
   "source": [
    "5. Configura√ß√£o do LoRA (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73095193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando LoRA (Low-Rank Adaptation)...\n",
      "Modelo configurado com adaptadores LoRA:\n",
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Configurando LoRA (Low-Rank Adaptation)...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "# Aplica a configura√ß√£o LoRA ao modelo\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Modelo configurado com adaptadores LoRA:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55959354",
   "metadata": {},
   "source": [
    "6. Defini√ß√£o dos Argumentos de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b743ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definindo argumentos de treinamento...\n",
      "Argumentos de treinamento definidos.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"./fine_tuned_llama\"\n",
    "\n",
    "print(\"Definindo argumentos de treinamento...\")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1, # Uma √∫nica √©poca de treinamento\n",
    "    per_device_train_batch_size=2, # REDUZIDO para 2 para economia de mem√≥ria\n",
    "    gradient_accumulation_steps=4, # AUMENTADO para 4 para manter o effective_batch_size\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    fp16=False,\n",
    "    bf16=True if device == \"mps\" else False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Argumentos de treinamento definidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02be04e",
   "metadata": {},
   "source": [
    "7. Inicializa√ß√£o do SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b5acb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando SFTTrainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58857/58857 [00:00<00:00, 61616.34 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/58857 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2207 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58857/58857 [00:11<00:00, 5076.67 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58857/58857 [00:00<00:00, 1515610.48 examples/s]\n",
      "Adding EOS to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3017/3017 [00:00<00:00, 55961.65 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3017/3017 [00:00<00:00, 5113.79 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3017/3017 [00:00<00:00, 1708182.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFTTrainer inicializado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Inicializando SFTTrainer...\")\n",
    "\n",
    "# Data Collator: Usado para agrupar e preencher os exemplos de dados em um batch.\n",
    "# Ele usar√° o tokenizer que j√° configuramos com model_max_length.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset, # Agora o dataset j√° est√° pr√©-formatado e cont√©m a coluna 'text'\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    # REMOVIDO: 'formatting_func' n√£o √© mais necess√°rio aqui, pois o dataset j√° foi formatado na C√©lula 3.\n",
    "    data_collator=data_collator,\n",
    "    args=training_arguments,\n",
    "    # REMOVIDO: 'max_seq_length' tamb√©m foi removido, pois o tokenizer e o data_collator devem lidar com isso.\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer inicializado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cdff0e",
   "metadata": {},
   "source": [
    "8. In√≠cio do Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f211a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o treinamento do modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7358' max='7358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7358/7358 61:04:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.865200</td>\n",
       "      <td>1.897628</td>\n",
       "      <td>1.918301</td>\n",
       "      <td>1788856.000000</td>\n",
       "      <td>0.609811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.907400</td>\n",
       "      <td>1.886621</td>\n",
       "      <td>1.897021</td>\n",
       "      <td>3576213.000000</td>\n",
       "      <td>0.611650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.850400</td>\n",
       "      <td>1.880466</td>\n",
       "      <td>1.874104</td>\n",
       "      <td>5372447.000000</td>\n",
       "      <td>0.612472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.839200</td>\n",
       "      <td>1.875961</td>\n",
       "      <td>1.869433</td>\n",
       "      <td>7158034.000000</td>\n",
       "      <td>0.613233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.867800</td>\n",
       "      <td>1.872083</td>\n",
       "      <td>1.887396</td>\n",
       "      <td>8939609.000000</td>\n",
       "      <td>0.613708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.885700</td>\n",
       "      <td>1.868985</td>\n",
       "      <td>1.876670</td>\n",
       "      <td>10730210.000000</td>\n",
       "      <td>0.614225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.850400</td>\n",
       "      <td>1.867453</td>\n",
       "      <td>1.875877</td>\n",
       "      <td>12552728.000000</td>\n",
       "      <td>0.614415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1058dc7d0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: b854f940-89cf-4943-b433-6876c20cd9a0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324ba6f00>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 039de909-64eb-445d-a3ba-af7a39711812)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324ba58b0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 094a8471-4bcc-44b3-9faa-455f438f170c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324e657c0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: d66bc30e-bd48-45cf-bd32-c63d2099e97f)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324e67560>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 09b1887f-e0f2-4b07-a692-d9e21380a43e)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324e65a60>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: e1d1969f-92a2-49a3-9332-e65417fdbd1e)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324e65a60>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: e1d1969f-92a2-49a3-9332-e65417fdbd1e)') - silently ignoring the lookup for the file config.json in TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
      "  warnings.warn(\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in TinyLlama/TinyLlama-1.1B-Chat-v1.0 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x32536dc40>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 550ca7c4-5863-447c-9a94-a85c898622d2)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x32536e0f0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: cc9250e0-b3d3-43e3-8b56-39d0e568961b)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x32536f740>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 79b6a0d0-2fc6-4261-973f-3e557ffeefd8)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x32536ffb0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: e8f38c5f-699e-408b-a2cb-d72f8ec335e8)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x32536d970>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: d97b0fbf-3501-4434-8a24-2bea38518d0a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324e67440>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 3cc64eb9-cfd1-4916-abbc-20bea3719819)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/peft/utils/other.py:1228: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x324e67440>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 3cc64eb9-cfd1-4916-abbc-20bea3719819)') - silently ignoring the lookup for the file config.json in TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
      "  warnings.warn(\n",
      "/Users/luizviana/tech_chall_3_v2/env/lib/python3.12/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in TinyLlama/TinyLlama-1.1B-Chat-v1.0 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento conclu√≠do!\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando o treinamento do modelo...\")\n",
    "trainer.train()\n",
    "print(\"Treinamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b4eb0",
   "metadata": {},
   "source": [
    "C√©lula 9 - Teste do modelo tunado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3555f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: mps\n",
      "Carregando modelo base 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' para infer√™ncia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tokenizer 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'...\n",
      "Tokenizer carregado e configurado.\n",
      "√öltimo checkpoint encontrado: ./fine_tuned_llama/checkpoint-7358\n",
      "Carregando adaptadores LoRA de './fine_tuned_llama/checkpoint-7358'...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo fine-tuned (com adaptadores LoRA) pronto para infer√™ncia!\n",
      "\n",
      "\n",
      "--- Testes R√°pidos do Modelo (ap√≥s C√©lula 9) ---\n",
      "\n",
      "Descri√ß√£o: 'Fone de ouvido sem fio com cancelamento de ru√≠do e bateria de longa dura√ß√£o.'\n",
      "T√≠tulo Gerado: '2 in 1 Battery / Loudspeaker Earphone - B&amp;O'\n",
      "\n",
      "Descri√ß√£o: 'Laptop leve e potente para trabalho e jogos, com tela de 15 polegadas e processador de √∫ltima gera√ß√£o.'\n",
      "T√≠tulo Gerado: '2012 Acer Aspire 7755T-7032G'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# Detec√ß√£o de dispositivo como na primeira c√©lula\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"Carregando modelo base '{model_id}' para infer√™ncia...\")\n",
    "\n",
    "# -- CORRE√á√ÉO PRINCIPAL AQUI --\n",
    "# 1. Carregar o modelo base SEM device_map=\"auto\" para evitar wrappers adicionais.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16 if device == \"mps\" else torch.float32,\n",
    "    # REMOVIDO: device_map=\"auto\" aqui.\n",
    ")\n",
    "# O modelo ser√° carregado na CPU por padr√£o (ou o dispositivo padr√£o do PyTorch para o dtype).\n",
    "\n",
    "print(f\"Carregando tokenizer '{model_id}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512\n",
    "print(\"Tokenizer carregado e configurado.\")\n",
    "\n",
    "base_output_dir = \"./fine_tuned_llama\"\n",
    "fine_tuned_model_path = base_output_dir\n",
    "\n",
    "# L√≥gica para encontrar o √∫ltimo checkpoint (a mais robusta)\n",
    "if os.path.exists(base_output_dir):\n",
    "    checkpoints = [\n",
    "        os.path.join(base_output_dir, d)\n",
    "        for d in os.listdir(base_output_dir)\n",
    "        if os.path.isdir(os.path.join(base_output_dir, d)) and d.startswith(\"checkpoint-\")\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        # Encontra o checkpoint com o maior n√∫mero de passos (o mais recente)\n",
    "        fine_tuned_model_path = max(checkpoints, key=lambda cp: int(cp.split('-')[-1]))\n",
    "        print(f\"√öltimo checkpoint encontrado: {fine_tuned_model_path}\")\n",
    "    else:\n",
    "        # Se nenhum subdiret√≥rio 'checkpoint-XXXX' for encontrado, tentamos o diret√≥rio base.\n",
    "        print(f\"Nenhum subdiret√≥rio 'checkpoint-XXXX' encontrado em '{base_output_dir}'. Tentando carregar diretamente de '{base_output_dir}'.\")\n",
    "        if not os.path.exists(os.path.join(base_output_dir, \"adapter_config.json\")):\n",
    "             print(f\"Erro: 'adapter_config.json' n√£o encontrado em '{base_output_dir}'. Verifique o diret√≥rio de sa√≠da do treinamento.\")\n",
    "             raise FileNotFoundError(f\"N√£o foi poss√≠vel encontrar o modelo LoRA em '{fine_tuned_model_path}'\")\n",
    "else:\n",
    "    print(f\"Diret√≥rio de sa√≠da '{base_output_dir}' n√£o encontrado. Verifique se o treinamento foi conclu√≠do e salvou os arquivos.\")\n",
    "    raise FileNotFoundError(f\"O diret√≥rio '{base_output_dir}' n√£o existe. Certifique-se de que o treinamento foi executado.\")\n",
    "\n",
    "\n",
    "print(f\"Carregando adaptadores LoRA de '{fine_tuned_model_path}'...\\n\")\n",
    "# Carrega os adaptadores PEFT e os anexa ao modelo base.\n",
    "# 'model' aqui √© o AutoModelForCausalLM puro, sem wrappers de device_map.\n",
    "model = PeftModel.from_pretrained(model, fine_tuned_model_path)\n",
    "\n",
    "# -- SEGUNDA CORRE√á√ÉO --\n",
    "# 2. Agora que o PeftModel foi criado corretamente (envolvendo o modelo base puro),\n",
    "# movemos TODO o PeftModel para o dispositivo.\n",
    "model.to(device)\n",
    "\n",
    "# A linha de merge_and_unload() continua removida, pois n√£o √© a causa raiz do erro e pode\n",
    "# ser problem√°tica em outros cen√°rios.\n",
    "# print(\"Mesclando adaptadores LoRA com o modelo base...\")\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "model.eval() # Coloca o modelo em modo de avalia√ß√£o\n",
    "\n",
    "print(\"Modelo fine-tuned (com adaptadores LoRA) pronto para infer√™ncia!\\n\")\n",
    "\n",
    "# Cria o pipeline de gera√ß√£o de texto\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, # Passamos o PeftModel diretamente\n",
    "    tokenizer=tokenizer,\n",
    "    # O device n√£o precisa ser explicitado aqui, pois o modelo j√° est√° no dispositivo correto.\n",
    "    # Se quiser ser expl√≠cito, pode usar: device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "\n",
    "def generate_product_title(description: str, max_new_tokens: int = 20) -> str:\n",
    "    # O prompt deve ser exatamente como foi formatado para o treinamento,\n",
    "    # at√© o ponto em que esperamos a gera√ß√£o do modelo.\n",
    "    # Exemplo de treinamento: <s>[INST] {content} [/INST] O t√≠tulo do produto que melhor corresponde a essa descri√ß√£o √©: {title}</s>\n",
    "    inference_prompt = (\n",
    "        f\"<s>[INST] Analise o seguinte coment√°rio/descri√ß√£o de produto: {description} [/INST] \"\n",
    "        f\"O t√≠tulo do produto que melhor corresponde a essa descri√ß√£o √©: \"\n",
    "    )\n",
    "\n",
    "    # Gerar a resposta\n",
    "    generated_output = text_generator(\n",
    "        inference_prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False # Esta op√ß√£o √© crucial para o pipeline retornar apenas o texto gerado ap√≥s o prompt\n",
    "    )\n",
    "\n",
    "    # O pipeline com return_full_text=False j√° retorna apenas a parte gerada.\n",
    "    # Precisamos apenas limpar os tokens especiais residuais.\n",
    "    if generated_output and len(generated_output) > 0:\n",
    "        generated_title = generated_output[0]['generated_text'].strip()\n",
    "    else:\n",
    "        return \"N√£o foi poss√≠vel gerar um t√≠tulo.\" # Em caso de falha na gera√ß√£o\n",
    "\n",
    "    # Remove quaisquer tokens de parada ou marca√ß√µes do prompt que possam ter sido gerados\n",
    "    # ou que o modelo \"ecoou\" e n√£o foram limpos automaticamente.\n",
    "    # Usamos split para garantir que pegamos apenas a parte antes do primeiro token de parada.\n",
    "    generated_title = generated_title.split(\"</s>\")[0].strip()\n",
    "    generated_title = generated_title.split(\"[/INST]\")[0].strip() # Para o caso de uma gera√ß√£o incompleta ou errada\n",
    "\n",
    "    return generated_title\n",
    "\n",
    "# --- Exemplos de Teste (para valida√ß√£o inicial da C√©lula 9) ---\n",
    "print(\"\\n--- Testes R√°pidos do Modelo (ap√≥s C√©lula 9) ---\\n\")\n",
    "\n",
    "desc1 = \"Fone de ouvido sem fio com cancelamento de ru√≠do e bateria de longa dura√ß√£o.\"\n",
    "title1 = generate_product_title(desc1)\n",
    "print(f\"Descri√ß√£o: '{desc1}'\\nT√≠tulo Gerado: '{title1}'\\n\")\n",
    "\n",
    "desc2 = \"Laptop leve e potente para trabalho e jogos, com tela de 15 polegadas e processador de √∫ltima gera√ß√£o.\"\n",
    "title2 = generate_product_title(desc2)\n",
    "print(f\"Descri√ß√£o: '{desc2}'\\nT√≠tulo Gerado: '{title2}'\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a7b0d",
   "metadata": {},
   "source": [
    "C√©lula 10 - Teste interativo para resposta de t√≠tulos de produtos a partir das caracter√≠sticas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets j√° est√° instalado.\n",
      "--- Interface Interativa para Testar o Modelo Tunado ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf162a404674bc7947934c07d6393da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Descri√ß√£o:', layout=Layout(height='100px', width='auto'), placeholder='Digite ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1141dc83ed6c421fb6092fb656ce1e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Gerar T√≠tulo', icon='magic', style=ButtonStyle(), tooltip='Clique ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffe80e150754a7daf013e4b24ebdaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instru√ß√µes de Uso:\n",
      "1. Digite a descri√ß√£o completa do produto na caixa de texto acima.\n",
      "2. Clique no bot√£o 'Gerar T√≠tulo'.\n",
      "3. O t√≠tulo gerado pelo seu modelo fine-tuned aparecer√° na √°rea de resultados abaixo do bot√£o.\n",
      "4. Para uma nova gera√ß√£o, apague ou edite a descri√ß√£o e clique novamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Cria o widget de entrada de texto para a descri√ß√£o do produto\n",
    "description_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Digite a descri√ß√£o do produto aqui...',\n",
    "    description='Descri√ß√£o:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='auto', height='100px') # Ajusta o layout para ocupar a largura total\n",
    ")\n",
    "\n",
    "# Cria um bot√£o para gerar o t√≠tulo\n",
    "generate_button = widgets.Button(\n",
    "    description='Gerar T√≠tulo',\n",
    "    disabled=False,\n",
    "    button_style='success', # Estilo visual do bot√£o (verde)\n",
    "    tooltip='Clique para gerar o t√≠tulo do produto',\n",
    "    icon='magic' # √çcone visual no bot√£o\n",
    ")\n",
    "\n",
    "# Cria um widget de sa√≠da para exibir o resultado e mensagens de carregamento\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# Fun√ß√£o que ser√° chamada quando o bot√£o for clicado\n",
    "def on_generate_button_clicked(b):\n",
    "    with output_widget:\n",
    "        clear_output()\n",
    "        description = description_input.value.strip()\n",
    "        \n",
    "        if not description:\n",
    "            print(\"Por favor, digite uma descri√ß√£o para o produto.\")\n",
    "            return\n",
    "\n",
    "        print(\"Gerando t√≠tulo... Isso pode levar alguns segundos. Por favor, aguarde.\")\n",
    "        try:\n",
    "            generated_title = generate_product_title(description)\n",
    "            print(f\"\\n--- Resultado da Gera√ß√£o ---\")\n",
    "            print(f\"Descri√ß√£o fornecida:\\n{description}\")\n",
    "            print(f\"\\nT√≠tulo Gerado:\\n{generated_title}\")\n",
    "            print(f\"\\n--- Fim do Resultado ---\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocorreu um erro durante a gera√ß√£o do t√≠tulo: {e}\")\n",
    "            print(\"Verifique se o modelo foi carregado corretamente na C√©lula 9 e se a fun√ß√£o generate_product_title est√° acess√≠vel.\")\n",
    "\n",
    "# Associa a fun√ß√£o ao evento de clique do bot√£o\n",
    "generate_button.on_click(on_generate_button_clicked)\n",
    "\n",
    "# Exibe os widgets na sa√≠da da c√©lula\n",
    "print(\"--- Interface Interativa para Testar o Modelo Tunado ---\")\n",
    "display(description_input, generate_button, output_widget)\n",
    "\n",
    "print(\"\\nInstru√ß√µes de Uso:\")\n",
    "print(\"1. Digite a descri√ß√£o completa do produto na caixa de texto acima.\")\n",
    "print(\"2. Clique no bot√£o 'Gerar T√≠tulo'.\")\n",
    "print(\"3. O t√≠tulo gerado pelo seu modelo fine-tuned aparecer√° na √°rea de resultados abaixo do bot√£o.\")\n",
    "print(\"4. Para uma nova gera√ß√£o, apague ou edite a descri√ß√£o e clique novamente.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
